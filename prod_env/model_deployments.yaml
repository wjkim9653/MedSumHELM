model_deployments:
  - name: huggingface/llama-3.1-8b-instruct
    model_name: meta/llama-3.1-8b-instruct
    tokenizer_name: meta/llama-3.1-8b-instruct
    max_sequence_length: 131072
    client_spec:
      class_name: "helm.clients.huggingface_client.HuggingFaceClient"
      args:
        pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
        torch_dtype: "float16"

  - name: huggingface/llama-3.2-3b-instruct
    model_name: meta/llama-3.2-3b-instruct
    tokenizer_name: meta/llama-3.2-3b-instruct
    max_sequence_length: 131072
    client_spec:
      class_name: "helm.clients.huggingface_client.HuggingFaceClient"
      args:
        pretrained_model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
        torch_dtype: "float16"

  - name: huggingface/llama-3.2-1b-instruct
    model_name: meta/llama-3.2-1b-instruct
    tokenizer_name: meta/llama-3.2-1b-instruct
    max_sequence_length: 131072
    client_spec:
      class_name: "helm.clients.huggingface_client.HuggingFaceClient"
      args:
        pretrained_model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
        torch_dtype: "float16"
    
  - name: openai/gpt-5-2025-08-07
    model_name: openai/gpt-5-2025-08-07
    tokenizer_name: openai/o200k_base
    max_sequence_length: 1047576
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"

  - name: openai/gpt-5-mini-2025-08-07
    model_name: openai/gpt-5-mini-2025-08-07
    tokenizer_name: openai/o200k_base
    max_sequence_length: 1047576
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"

  - name: openai/gpt-4.1-2025-04-14
    model_name: openai/gpt-4.1-2025-04-14
    tokenizer_name: openai/o200k_base
    max_sequence_length: 1047576
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"

  - name: openai/gpt-4.1-mini-2025-04-14
    model_name: openai/gpt-4.1-mini-2025-04-14
    tokenizer_name: openai/o200k_base
    max_sequence_length: 1047576
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"
    
  - name: openai/gpt-4.1-nano-2025-04-14
    model_name: openai/gpt-4.1-nano-2025-04-14
    tokenizer_name: openai/o200k_base
    max_sequence_length: 1047576
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"

  - name: anthropic/claude-3-7-sonnet-20250219
    model_name: anthropic/claude-3-7-sonnet-20250219
    tokenizer_name: anthropic/claude
    max_sequence_length: 200000
    client_spec:
      class_name: "helm.clients.anthropic_client.AnthropicMessagesClient"

  - name: anthropic/claude-opus-4-1-20250805
    model_name: anthropic/claude-opus-4-1-20250805
    tokenizer_name: anthropic/claude
    max_sequence_length: 200000
    client_spec:
      class_name: "helm.clients.anthropic_client.AnthropicMessagesClient"

  - name: together/deepseek-v3
    model_name: deepseek-ai/deepseek-v3
    tokenizer_name: deepseek-ai/deepseek-v3
    max_sequence_length: 16384
    client_spec:
      class_name: "helm.clients.together_client.TogetherChatClient"
      args:
        disable_logprobs: True

  - name: together/deepseek-r1-0528
    model_name: deepseek-ai/deepseek-r1-0528
    tokenizer_name: deepseek-ai/deepseek-r1
    max_sequence_length: 32768
    client_spec:
      class_name: "helm.clients.together_client.TogetherChatClient"
      args:
        together_model: deepseek-ai/deepseek-r1
        parse_thinking: true
        disable_logprobs: True
  
  - name: together/llama-3.3-70b-instruct-turbo
    model_name: meta/llama-3.3-70b-instruct-turbo
    tokenizer_name: meta/llama-3.3-70b-instruct
    max_sequence_length: 128000
    client_spec:
      class_name: "helm.clients.together_client.TogetherChatClient"
      args:
        together_model: meta-llama/Llama-3.3-70B-Instruct-Turbo-Free
   
  - name: gemini/gemini-2.0-flash
    model_name: gemini/gemini-2.0-flash
    tokenizer_name: google/gemma-2b  # Gemini has no tokenizer endpoint, so we approximate by using Gemma's tokenizer.
    max_sequence_length: 1000000  # Source: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models
    # TODO: Max output tokens: 8192
    client_spec:
      class_name: "helm.clients.gemini_client.GeminiClient"