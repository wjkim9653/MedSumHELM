[
  {
    "title": "",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "Jury Score",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\nMTSamples Replicate Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "Jury Score",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "Observed inference time (s)",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Observed inference time (s)",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "# eval",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "# train",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "truncated",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MTSamples"
        }
      },
      {
        "value": "# output tokens",
        "description": "MTSamples Replicate is a benchmark that provides transcribed medical reports from various specialties. It is used to evaluate a model's ability to generate clinically appropriate treatment plans based on unstructured patient documentation [(MTSamples, 2025)](https://mtsamples.com).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MTSamples"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "GPT-4.1-nano (2025-04-14)",
          "description": "",
          "href": "?group=mtsamples_replicate&subgroup=&runSpecs=%5B%22mtsamples_replicate%3Amodel%3Dopenai_gpt-4.1-nano-2025-04-14%2Cmodel_deployment%3Dopenai_gpt-4.1-nano-2025-04-14%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mtsamples_replicate:model=openai_gpt-4.1-nano-2025-04-14,model_deployment=openai_gpt-4.1-nano-2025-04-14"
          ]
        },
        {
          "value": 3.866666666666667,
          "description": "min=3.867, mean=3.867, max=3.867, sum=3.867 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 6.039678835868836,
          "description": "min=6.04, mean=6.04, max=6.04, sum=6.04 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 10.0,
          "description": "min=10, mean=10, max=10, sum=10 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 873.0,
          "description": "min=873, mean=873, max=873, sum=873 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 487.0,
          "description": "min=487, mean=487, max=487, sum=487 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/my-medhelm-suite/groups/latex/mtsamples_replicate_mtsamples_replicate_.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/my-medhelm-suite/groups/json/mtsamples_replicate_mtsamples_replicate_.json"
      }
    ],
    "name": "mtsamples_replicate_"
  }
]