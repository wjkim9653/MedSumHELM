{
  "title": "Accuracy",
  "header": [
    {
      "value": "Model",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Mean win rate",
      "description": "How many models this model outperforms on average (over columns).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {}
    },
    {
      "value": "DischargeMe - Jury Score",
      "description": "DischargeMe is a benchmark designed to evaluate clinical text generation. It pairs discharge summaries and radiology reports from MIMIC-IV with generation tasks such as writing discharge instructions or summarizing the brief hospital course. The benchmark assesses a model's ability to generate patient-facing documentation that is complete, empathetic, and clinically accurate [(Xu, 2024)](https://physionet.org/content/discharge-me/1.3/).\n\nDischargeMe Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "DischargeMe"
      }
    },
    {
      "value": "ACI-Bench - Jury Score",
      "description": "ACI-Bench is a benchmark of real-world patient-doctor conversations paired with structured clinical notes. The benchmark evaluates a model's ability to understand spoken medical dialogue and convert it into formal clinical documentation, covering sections such as history of present illness, physical exam findings, results, and assessment and plan [(Yim et al., 2024)](https://www.nature.com/articles/s41597-023-02487-3).\n\nACI-Bench Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "ACI-Bench"
      }
    },
    {
      "value": "MTSamples Procedures - Jury Score",
      "description": "MTSamples Procedures is a benchmark composed of transcribed operative notes, focused on documenting surgical procedures. Each example presents a brief patient case involving a surgical intervention, and the model is tasked with generating a coherent and clinically accurate procedural summary or treatment plan.\n\nMTSamples Procedures Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "MTSamples Procedures"
      }
    },
    {
      "value": "MIMIC-RRS - Jury Score",
      "description": "MIMIC-RRS is a benchmark constructed from radiology reports in the MIMIC-III database. It contains pairs of \u2018Findings\u2018 and \u2018Impression\u2018 sections, enabling evaluation of a model's ability to summarize diagnostic imaging observations into concise, clinically relevant conclusions [(Chen et al., 2023)](https://arxiv.org/abs/2211.08584).\n\nMIMIC-RRS Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "MIMIC-RRS"
      }
    },
    {
      "value": "MIMIC-BHC - Jury Score",
      "description": "MIMIC-BHC is a benchmark focused on summarization of discharge notes into Brief Hospital Course (BHC) sections. It consists of curated discharge notes from MIMIC-IV, each paired with its corresponding BHC summary. The benchmark evaluates a model's ability to condense detailed clinical information into accurate, concise summaries that reflect the patient's hospital stay [(Aali et al., 2024)](https://doi.org/10.1093/jamia/ocae312).\n\nMIMIC-BHC Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "MIMIC-BHC"
      }
    },
    {
      "value": "NoteExtract - Jury Score",
      "description": "NoteExtract is a benchmark that focuses on the structured extraction of information from free-form clinical text. It provides care plan notes authored by health workers and evaluates a model's ability to convert them into a predefined structured format, such as fields for Chief Complaint and History of Present Illness. The benchmark emphasizes faithful extraction without hallucination or inference.\n\nNoteExtract Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "NoteExtract"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "Llama 3.1 Instruct (8B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.25,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 3.7999999999999994,
        "description": "min=3.8, mean=3.8, max=3.8, sum=3.8 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=meta_llama-3.1-8b-instruct,model_deployment=huggingface_llama-3.1-8b-instruct"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Llama 3.2 Instruct (1.23B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.0,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 1.9000000000000004,
        "description": "min=1.9, mean=1.9, max=1.9, sum=1.9 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=meta_llama-3.2-1b-instruct,model_deployment=huggingface_llama-3.2-1b-instruct"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Llama 3.2 Instruct (3B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.125,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 3.3333333333333335,
        "description": "min=3.333, mean=3.333, max=3.333, sum=3.333 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=meta_llama-3.2-3b-instruct,model_deployment=huggingface_llama-3.2-3b-instruct"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "DeepSeek-R1-0528",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.75,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.433333333333334,
        "description": "min=4.433, mean=4.433, max=4.433, sum=4.433 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:num_output_tokens=4000,model=deepseek-ai_deepseek-r1-0528,model_deployment=together_deepseek-r1-0528"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Llama 3.3 Instruct Turbo (70B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.5,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.166666666666667,
        "description": "min=4.167, mean=4.167, max=4.167, sum=4.167 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=meta_llama-3.3-70b-instruct-turbo,model_deployment=together_llama-3.3-70b-instruct-turbo"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Claude 3.7 Sonnet (20250219)",
        "description": "",
        "markdown": false
      },
      {
        "value": 1.0,
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.7,
        "description": "min=4.7, mean=4.7, max=4.7, sum=4.7 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=anthropic_claude-3-7-sonnet-20250219,model_deployment=anthropic_claude-3-7-sonnet-20250219"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-4.1 (2025-04-14)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.875,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.533333333333333,
        "description": "min=4.533, mean=4.533, max=4.533, sum=4.533 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=openai_gpt-4.1-2025-04-14,model_deployment=openai_gpt-4.1-2025-04-14"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-4.1-mini (2025-04-14)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.625,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.4,
        "description": "min=4.4, mean=4.4, max=4.4, sum=4.4 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=openai_gpt-4.1-mini-2025-04-14,model_deployment=openai_gpt-4.1-mini-2025-04-14"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "GPT-4.1-nano (2025-04-14)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.375,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 4.1,
        "description": "min=4.1, mean=4.1, max=4.1, sum=4.1 (1)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "aci_bench:model=openai_gpt-4.1-nano-2025-04-14,model_deployment=openai_gpt-4.1-nano-2025-04-14"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/latex/clinical_note_generation_accuracy.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/json/clinical_note_generation_accuracy.json"
    }
  ],
  "name": "accuracy"
}